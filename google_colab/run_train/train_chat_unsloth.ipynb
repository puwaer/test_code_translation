{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "googleドライブ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall unsloth -y\n",
    "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --upgrade torch\n",
    "!pip install --upgrade xformers\n",
    "\n",
    "import torch\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "max_seq_length = 512                                    # コンテキスト長\n",
    "dtype = None                                            # Noneにしておけば自動で設定\n",
    "load_in_4bit = True                                     # 量子化 True\n",
    "\n",
    "model_id = \"llm-jp/llm-jp-3-3.7b\"\n",
    "new_model_id = \"test_1\" \n",
    "dataset = load_dataset(\"json\", data_files=\"/content/ichikara-instruction-003-001-test.json\")\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, #デフォルト32\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,                                                #デフォルト32\n",
    "    lora_dropout = 0.05,                                            #デフォルト0.05\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,                                            #デフォルト3407\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "\n",
    "# 学習時のプロンプトフォーマットの定義\n",
    "prompt = \"\"\"### 指示\n",
    "{}\n",
    "### 回答\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # トークナイザーのEOSトークン（文末トークン）\n",
    "def formatting_prompts_func(examples):\n",
    "    input = examples[\"text\"] # 入力データ\n",
    "    output = examples[\"output\"] # 出力データ\n",
    "    text = prompt.format(input, output) + EOS_TOKEN # プロンプトの作成\n",
    "    return { \"formatted_text\" : text, } # 新しいフィールド \"formatted_text\" を返す\n",
    "pass\n",
    "\n",
    "# # 各データにフォーマットを適用\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    num_proc= 4,                      # 並列処理数を指定\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training_arguments: 学習の設定\n",
    "\n",
    "  - output_dir:\n",
    "      -トレーニング後のモデルを保存するディレクトリ\n",
    "\n",
    "  - per_device_train_batch_size:\n",
    "      - デバイスごとのトレーニングバッチサイズ\n",
    "\n",
    "  - per_device_eval_batch_size:\n",
    "      - デバイスごとの評価バッチサイズ\n",
    "\n",
    "  - gradient_accumulation_steps:\n",
    "      - 勾配を更新する前にステップを積み重ねる回数\n",
    "\n",
    "  - optim:\n",
    "      - オプティマイザの設定\n",
    "\n",
    "  - num_train_epochs:\n",
    "      - エポック数\n",
    "\n",
    "  - eval_strategy:\n",
    "      - 評価の戦略 (\"no\"/\"steps\"/\"epoch\")\n",
    "\n",
    "  - eval_steps:\n",
    "      - eval_strategyが\"steps\"のとき、評価を行うstep間隔\n",
    "\n",
    "  - logging_strategy:\n",
    "      - ログ記録の戦略\n",
    "\n",
    "  - logging_steps:\n",
    "      - ログを出力するステップ間隔\n",
    "\n",
    "  - warmup_steps:\n",
    "      - 学習率のウォームアップステップ数\n",
    "\n",
    "  - save_steps:\n",
    "      - モデルを保存するステップ間隔\n",
    "\n",
    "  - save_total_limit:\n",
    "      - 保存しておくcheckpointの数\n",
    "\n",
    "  - max_steps:\n",
    "      - トレーニングの最大ステップ数\n",
    "\n",
    "  - learning_rate:\n",
    "      - 学習率\n",
    "\n",
    "  - fp16:\n",
    "      - 16bit浮動小数点の使用設定（第8回演習を参考にすると良いです）\n",
    "\n",
    "  - bf16:\n",
    "      - BFloat16の使用設定\n",
    "\n",
    "  - group_by_length:\n",
    "      -  入力シーケンスの長さによりバッチをグループ化 (トレーニングの効率化)\n",
    "\n",
    "  - report_to:\n",
    "      - ログの送信先 (\"wandb\"/\"tensorboard\"など)\n",
    "\"\"\"\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_text_field=\"formatted_text\",\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        logging_steps = 10,\n",
    "        warmup_steps = 10,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        max_steps=-1,\n",
    "        learning_rate = 2e-4, #デフォルト2e-4\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        group_by_length=True,\n",
    "        seed = 3407, #デフォルト3407\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習、保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# LoRAアダプタだけ保存\n",
    "model.push_to_hub_merged(\n",
    "    new_model_id+\"_lora\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"lora\",\n",
    "    token=HF_TOKEN,\n",
    "    private=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
