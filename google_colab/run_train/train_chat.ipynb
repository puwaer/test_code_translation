{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 依存関係のインストール\n",
    "!pip install numpy==1.26.4 torch==2.6.0 transformers==4.49.0 tokenizers==0.21.0 accelerate==1.4.0 trl==0.15.2 peft==0.14.0 sentencepiece==0.2.0 wandb deepspeed==0.16.4 bitsandbytes==0.45.3 scipy==1.15.2\n",
    "!pip install datasets\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Google Driveをマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# データとモデルのパスを設定（適宜変更）\n",
    "DATA_FILE_PATH = '/content/drive/MyDrive/program/code_translation/data/sample_data.jsonl\n",
    "MODEL_PATH = '/content/drive/MyDrive/program/code_translation/base_model/llm-jp-3-1.8b'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/program/code_translation/model/test_model_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights & Biasesにログイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_chat.pyのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from datasets import disable_caching, load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "disable_caching()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class SFTTrainingArguments:\n",
    "    model_name_or_path: str\n",
    "    data_files: List[str]\n",
    "    eval_data_files: Optional[List[str]] = None\n",
    "    tokenizer_name_or_path: Optional[str] = None\n",
    "    use_fast: bool = True\n",
    "    additional_special_tokens: Optional[List[str]] = None\n",
    "    max_seq_length: int = 4096\n",
    "    load_in_8bit: bool = False\n",
    "    load_in_4bit: bool = False\n",
    "    use_flash_attention_2: bool = False\n",
    "    use_peft: bool = False\n",
    "    peft_target_model: Optional[str] = \"llm-jp\"\n",
    "    peft_target_modules: Optional[List[str]] = None\n",
    "    peft_lora_r: int = 8\n",
    "    peft_lora_alpha: int = 32\n",
    "    peft_lora_dropout: float = 0.05\n",
    "    wandb_project: Optional[str] = None\n",
    "    wandb_run_name: Optional[str] = None\n",
    "    wandb_log_steps: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.load_in_8bit and self.load_in_4bit:\n",
    "            raise ValueError(\"load_in_8bit and load_in_4bit are mutually exclusive\")\n",
    "        if self.peft_target_model and self.peft_target_modules is None:\n",
    "            if self.peft_target_model == \"llm-jp\":\n",
    "                self.peft_target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "            elif self.peft_target_model == \"llama\":\n",
    "                self.peft_target_modules = [\n",
    "                    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "                ]\n",
    "            elif self.peft_target_model == \"llama-all\":\n",
    "                self.peft_target_modules = [\n",
    "                    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\", \"embed_tokens\"\n",
    "                ]\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"peft_target_model '{self.peft_target_model}' is not supported, \"\n",
    "                    f\"so peft_target_modules is set to None.\"\n",
    "                )\n",
    "\n",
    "    def from_pretrained_kwargs(self, training_args):\n",
    "        if self.load_in_8bit:\n",
    "            kwargs = {\"load_in_8bit\": True}\n",
    "        elif self.load_in_4bit:\n",
    "            kwargs = {\n",
    "                \"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                )\n",
    "            }\n",
    "        elif training_args.bf16:\n",
    "            kwargs = {\"torch_dtype\": torch.bfloat16}\n",
    "        else:\n",
    "            kwargs = {\"torch_dtype\": torch.float16}\n",
    "        kwargs[\"use_flash_attention_2\"] = self.use_flash_attention_2\n",
    "        return kwargs\n",
    "\n",
    "def load_datasets(data_files, tokenizer, max_seq_length=2048):\n",
    "    datasets = []\n",
    "    for data_file in data_files:\n",
    "        dataset = load_dataset(\"json\", data_files=data_file)\n",
    "        dataset = dataset[\"train\"]\n",
    "\n",
    "        def tokenize_function(example):\n",
    "            tokenized = tokenizer.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=False,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_dict=True\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokenized[\"input_ids\"].squeeze(0).tolist(),\n",
    "                \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0).tolist()\n",
    "            }\n",
    "\n",
    "        dataset = dataset.map(tokenize_function, remove_columns=dataset.column_names)\n",
    "        datasets.append(dataset)\n",
    "    return concatenate_datasets(datasets)\n",
    "\n",
    "def main():\n",
    "    # トレーニング引数の設定\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        report_to=[\"wandb\"],  # Weights & Biasesにログを送信\n",
    "    )\n",
    "\n",
    "    # SFTトレーニング引数の設定\n",
    "    sft_training_args = SFTTrainingArguments(\n",
    "        model_name_or_path=MODEL_PATH,\n",
    "        data_files=[DATA_FILE_PATH],\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=True,  # ColabのGPUメモリ節約のため4bitロードを使用\n",
    "        use_flash_attention_2=True,  # Flash Attentionを使用（対応する場合）\n",
    "        wandb_project=\"puwaer-sft\",\n",
    "        wandb_run_name=\"doujinshi-1.8b-instruct-1\",\n",
    "        wandb_log_steps=10,\n",
    "    )\n",
    "\n",
    "    # W&Bの初期化\n",
    "    wandb.init(\n",
    "        project=sft_training_args.wandb_project,\n",
    "        name=sft_training_args.wandb_run_name,\n",
    "        config=vars(training_args),\n",
    "    )\n",
    "\n",
    "    # トークナイザーのロード\n",
    "    tokenizer_name_or_path = sft_training_args.tokenizer_name_or_path or sft_training_args.model_name_or_path\n",
    "    logger.info(f\"Loading tokenizer from {tokenizer_name_or_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=sft_training_args.use_fast,\n",
    "        additional_special_tokens=sft_training_args.additional_special_tokens,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # チャットテンプレートの設定\n",
    "    chat_template = (\n",
    "        \"{{bos_token}}{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}{{ '\\\\n\\\\n### 指示:\\\\n' + message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'system' %}{{ '以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。' }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}{{ '\\\\n\\\\n### 応答:\\\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% if loop.last and add_generation_prompt %}{{ '\\\\n\\\\n### 応答:\\\\n' }}{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "    )\n",
    "    tokenizer.chat_template = chat_template\n",
    "    logger.info(\"Custom chat template applied to tokenizer\")\n",
    "\n",
    "    if tokenizer.bos_token is None:\n",
    "        tokenizer.bos_token = \"<|begin_of_text|>\"\n",
    "        logger.info(f\"Set default bos_token: {tokenizer.bos_token}\")\n",
    "    if tokenizer.eos_token is None:\n",
    "        tokenizer.eos_token = \"<|end_of_text|>\"\n",
    "        logger.info(f\"Set default eos_token: {tokenizer.eos_token}\")\n",
    "\n",
    "    # データのロード\n",
    "    logger.info(\"Loading data\")\n",
    "    train_dataset = load_datasets(sft_training_args.data_files, tokenizer, sft_training_args.max_seq_length)\n",
    "\n",
    "    # モデルのロード\n",
    "    logger.info(f\"Loading model from {sft_training_args.model_name_or_path}\")\n",
    "    kwargs = sft_training_args.from_pretrained_kwargs(training_args)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        sft_training_args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # トレーナーの設定\n",
    "    logger.info(\"Setting up trainer\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # 評価データがない場合\n",
    "    )\n",
    "\n",
    "    # トレーニングの実行\n",
    "    logger.info(\"Training\")\n",
    "    trainer.train()\n",
    "\n",
    "    # モデルの保存\n",
    "    logger.info(\"Saving model\")\n",
    "    trainer.save_model()\n",
    "\n",
    "    # W&Bの終了\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s %(name)s:%(lineno)d: %(levelname)s: %(message)s\",\n",
    "    )\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
