{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "googleドライブ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Secretsからトークンを取得\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -U torch\n",
    "!pip install -U peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "\n",
    "#model_id = \"llm-jp/llm-jp-3-13b\"\n",
    "model_id = \"/content/drive/MyDrive/program/code_translation/base_model/llm-jp-3-1.8b\"\n",
    "model_adapter = \"test_2\"\n",
    "#adapter_id = \"puwaer/llm-jp-3-13b-it_lora\"\n",
    "adapter_id = f\"/content/drive/MyDrive/program/code_translation/model_adapter/{model_adapter}\"\n",
    "\n",
    "#benchmark_data = \"./elyza-tasks-100-TV_test.jsonl\"\n",
    "benchmark_data = \"/content/drive/MyDrive/program/code_translation/data/elyza-tasks-100-TV_test.jsonl\"\n",
    "\n",
    "\n",
    "dtype = None                                                # Noneにしておけば自動で設定\n",
    "load_in_4bit = True                                         # 今回は13Bモデルを扱うためTrue\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "#model = PeftModel.from_pretrained(model, adapter_id, token = HF_TOKEN)\n",
    "model = PeftModel.from_pretrained(model, adapter_id, token = HF_TOKEN)\n",
    "\n",
    "\n",
    "datasets = []\n",
    "with open(benchmark_data, \"r\") as f:\n",
    "    item = \"\"\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      item += line\n",
    "      if item.endswith(\"}\"):\n",
    "        datasets.append(json.loads(item))\n",
    "        item = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論するためにモデルのモードを変更\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "results = []\n",
    "for dt in tqdm(datasets):\n",
    "  input = dt[\"input\"]\n",
    "\n",
    "  prompt = f\"\"\"### 指示\\n{input}\\n### 回答\\n\"\"\"\n",
    "\n",
    "  inputs = tokenizer([prompt], return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n",
    "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### 回答')[-1]\n",
    "\n",
    "  results.append({\"task_id\": dt[\"task_id\"], \"input\": input, \"output\": prediction})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をjsonlで保存。\n",
    "\n",
    "# ここではadapter_idを元にファイル名を決定しているが、ファイル名は任意で問題なし。\n",
    "json_file_id = re.sub(\".*/\", \"\", adapter_id)\n",
    "with open(f\"/content/{json_file_id}_output.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        json.dump(result, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
